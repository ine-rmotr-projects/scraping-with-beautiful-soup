{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ine-divider](https://user-images.githubusercontent.com/7065401/92672068-398e8080-f2ee-11ea-82d6-ad53f7feb5c0.png)\n",
    "<hr>\n",
    "\n",
    "# Web scraping in Python\n",
    "\n",
    "## Beautiful Soup\n",
    "\n",
    "In this project, you will use Beautiful Soup to scrape content from additional websites.  For this purpose, use the `requests` third party module to actually obtain the web page contents.  Beautiful Soup will be useful for processing and extracting parts of interest.\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "**A fictional bookstore**\n",
    "\n",
    "The URL http://books.toscrape.com/ contains a collection of pages that resemble an online bookstore.  Prices and ratings are randomly assigned by them.  The book titles and authors appear to be actual books, although I have not verified all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first task, identify all the \"Autobiography\" title and their prices.  Save this information in a Python dictionary, or if you are familiar with Pandas, in a Pandas DataFrame.  Ideally, for this exercise, your web crawler will begin with the home page, and navigate within pages programmatically (i.e. do not manually find nested URLs).  \n",
    "\n",
    "As with other web scraping tasks, getting the steps right will certainly require some trial-and-error, and examination of partial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://books.toscrape.com/'\n",
    "bookstore = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(bookstore.text)\n",
    "sidebar = soup.find('aside')\n",
    "for a in sidebar.find_all('a'):\n",
    "    if 'autobiography' in a.text.lower():\n",
    "        url_autobio = url + a['href']\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url_autobio).text\n",
    "autobiography = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for title in autobiography.find_all('a'):\n",
    "    if title.get('title'):\n",
    "        titles.append(title['title']) \n",
    "        \n",
    "prices = []\n",
    "for price in autobiography(class_=\"product_price\"):\n",
    "    # Stray character appears before the pound symbol\n",
    "    prices.append(price.p.text.replace('Â', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Argonauts</td>\n",
       "      <td>£10.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M Train</td>\n",
       "      <td>£27.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lab Girl</td>\n",
       "      <td>£40.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Approval Junkie: Adventures in Caring Too Much</td>\n",
       "      <td>£58.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Running with Scissors</td>\n",
       "      <td>£12.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Me Talk Pretty One Day</td>\n",
       "      <td>£57.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lust &amp; Wonder</td>\n",
       "      <td>£11.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Life Without a Recipe</td>\n",
       "      <td>£59.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A Heartbreaking Work of Staggering Genius</td>\n",
       "      <td>£54.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Title   Price\n",
       "0                                   The Argonauts  £10.93\n",
       "1                                         M Train  £27.18\n",
       "2                                        Lab Girl  £40.85\n",
       "3  Approval Junkie: Adventures in Caring Too Much  £58.81\n",
       "4                           Running with Scissors  £12.91\n",
       "5                          Me Talk Pretty One Day  £57.60\n",
       "6                                   Lust & Wonder  £11.87\n",
       "7                           Life Without a Recipe  £59.04\n",
       "8       A Heartbreaking Work of Staggering Genius  £54.29"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "autobiographies = pd.DataFrame(list(zip(titles, prices)),\n",
    "                               columns=['Title', 'Price'])\n",
    "autobiographies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)\n",
    "\n",
    "## Part 2\n",
    "\n",
    "**NOAA factoids**\n",
    "\n",
    "The website for the United States National Oceanic and Atmospheric Administration (https://www.noaa.gov/) contains a sidebar on the left.  The links in the sidebar lead to subject-area sections like \"Fisheries\" and \"Satellites.\"  Each of those contains a large \"quick fact\" that is thematically related to that area.  For example:\n",
    "\n",
    "<img src=\"img/NOAA-fact.png\" width=\"50%\" />\n",
    "\n",
    "The particular quotes provided are randomized, and there is a link to cause a new quote to appear (still as relevant to the section).\n",
    "\n",
    "For this task, pull one quote from each sidebar link, if the corresponding page has a quote.  Ignore the extraction if the page lacks such a quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.noaa.gov/'\n",
    "noaa = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(noaa.text)\n",
    "sidebar = soup.find(id=\"navigation-main\")\n",
    "sections = [e.a['href'] for e in sidebar.find_all(class_=\"leaf\")]\n",
    "urls = [url+section for section in sections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3 billion observations per day:\n",
      "Each day, NWS collects about 6.3 billion observations. The weather service annually issues approximately 1.5 million forecasts and 50,000 warnings.\n",
      "-----\n",
      "2019 was second-warmest year on record:\n",
      "Earth’s global average surface temperature was 1.71°F (0.95°C) above the 20th-century average in 2019. Nine of the 10 warmest years on record have occurred since 2005.\n",
      "-----\n",
      "More than 80% unexplored:\n",
      "Most of the ocean is unseen by human eyes -- more than 80% of our ocean is unmapped, unobserved and unexplored.\n",
      "-----\n",
      "47:\n",
      "The number of fish stocks rebuilt since 2000 as a result of our fishery management process. The number of stocks on the overfishing list dropped to an all-time low in 2019.\n",
      "-----\n",
      "Orbiting 1 million miles from Earth:\n",
      "DSCOVR, NOAA’s first operational satellite in deep space, orbits a million miles from Earth in order to provide early warnings of potentially harmful space weather.\n",
      "-----\n",
      "20 :\n",
      "The number of NOAA Cooperatative Institutes (CIs). Partnerships CIs, which are located at degree-granting institutions, play a vital role in increasing NOAA’s research capacity and expertise. CIs support NOAA's mission and educate the next generation of the nation’s scientific workforce to prepare NOAA for the future.\n",
      "-----\n",
      "More than 650:\n",
      "The number of data collection flights the NOAA Twin Otter planes completed throughout the course of 2019.\n",
      "-----\n",
      "More than $2.4 billion in annual benefits to U.S. economy:\n",
      "The NOAA National Spatial Reference System—the official U.S. government source for precise latitude, longitude, and elevation measurements—provides more than $2.4 billion in potential annual benefits to the U.S. economy.\n",
      "-----\n",
      "Apalachicola Bay is referred to as the “Oyster Capital of the World”:\n",
      "Apalachicola Bay contributes about 90% of the state of Florida’s oysters and 10% for the U.S.\n",
      "-----\n",
      "61.4 million:\n",
      "people visited informal education institutions hosting NOAA-supported exhibits or programs in fiscal year 2019.\n",
      "-----\n",
      "146 million miles:\n",
      "The distance one of NOAA's polar-orbiting satellites travels in one year.:\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for url in urls:\n",
    "    topic = BeautifulSoup(requests.get(url).text)\n",
    "    fact = topic.find(class_='field-collection-item-field-quick-facts')\n",
    "    if fact:\n",
    "        fact = re.sub(r\"[\\r\\n]+\", \":\\n\", fact.text.strip())\n",
    "        print(fact)\n",
    "        print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
