{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ine-divider](https://user-images.githubusercontent.com/7065401/92672068-398e8080-f2ee-11ea-82d6-ad53f7feb5c0.png)\n",
    "<hr>\n",
    "\n",
    "# Web scraping in Python\n",
    "\n",
    "## Beautiful Soup\n",
    "\n",
    "In this project, you will use Beautiful Soup to scrape content from additional websites.  For this purpose, use the `requests` third party module to actually obtain the web page contents.  Beautiful Soup will be useful for processing and extracting parts of interest.\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "**A fictional bookstore**\n",
    "\n",
    "The URL http://books.toscrape.com/ contains a collection of pages that resemble an online bookstore.  Prices and ratings are randomly assigned by them.  The book titles and authors appear to be actual books, although I have not verified all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first task, identify all the \"Autobiography\" title and their prices.  Save this information in a Python dictionary, or if you are familiar with Pandas, in a Pandas DataFrame.  Ideally, for this exercise, your web crawler will begin with the home page, and navigate within pages programmatically (i.e. do not manually find nested URLs).  \n",
    "\n",
    "As with other web scraping tasks, getting the steps right will certainly require some trial-and-error, and examination of partial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://books.toscrape.com/'\n",
    "bookstore = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)\n",
    "\n",
    "## Part 2\n",
    "\n",
    "**NOAA factoids**\n",
    "\n",
    "The website for the United States National Oceanic and Atmospheric Administration (https://www.noaa.gov/) contains a sidebar on the left.  The links in the sidebar lead to subject-area sections like \"Fisheries\" and \"Satellites.\"  Each of those contains a large pull quote that is thematically related to that area.  For example:\n",
    "\n",
    "<img src=\"img/NOAA-fact.png\" width=\"50%\" />\n",
    "\n",
    "The particular quotes provided are randomized, and there is a link to cause a new quote to appear (still as relevant to the section).\n",
    "\n",
    "For this task, pull one quote from each sidebar link, if the corresponding page has a quote.  Ignore the extraction if the page lacks such a quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.noaa.gov/'\n",
    "noaa = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
